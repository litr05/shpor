#################################  Installing kubeadm, kubelet and kubectl ###########################################
proxy

apt-get update
apt-get install -y apt-transport-https ca-certificates curl

curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list

apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl


################################## ETCD ##########################################

mkdir archives
cd archives
export etcdVersion=v3.5.1
wget https://github.com/coreos/etcd/releases/download/$etcdVersion/etcd-$etcdVersion-linux-amd64.tar.gz
tar -xvf etcd-$etcdVersion-linux-amd64.tar.gz -C /usr/local/bin/ --strip-components=1


groupadd --system etcd
useradd -s /sbin/nologin --system -g etcd etcd

mkdir -p /var/lib/etcd/
mkdir /etc/etcd
chown -R etcd:etcd /var/lib/etcd/
chmod -R 700 /var/lib/etcd/

INT_NAME="ens32"
ETCD_HOST_IP=$(ip addr show $INT_NAME | grep "inet\b" | awk '{print $2}' | cut -d/ -f1)
ETCD_NAME=$(hostname -s)

echo $INT_NAME
echo $ETCD_HOST_IP
echo $ETCD_NAME

cat <<EOF | tee /etc/systemd/system/etcd.service
[Unit]
Description=etcd service
Documentation=https://github.com/etcd-io/etcd

[Service]
Type=notify
User=etcd
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --data-dir=/var/lib/etcd \\
  --listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 \\
  --advertise-client-urls http://${ETCD_HOST_IP}:2379 \\
  --listen-peer-urls http://0.0.0.0:2380 \\
  --initial-advertise-peer-urls http://${ETCD_HOST_IP}:2380 \\
  --initial-cluster ${ETCD_NAME}=http://${ETCD_HOST_IP}:2380 \\
  --initial-cluster-token etcd-0 \\
  --initial-cluster-state new \

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
systemctl daemon-reload && systemctl restart etcd

etcdctl -w table member list
curl -sL http://10.40.254.133:2379/metrics | grep etcd_server_is_leader

ENDPOINTS=10.40.1.129:2379,10.40.1.140:2379,10.40.1.149:2379
ENDPOINTS=192.168.2.11:2379

ENDPOINTS=10.40.1.129:2379
ETCDCTL_DIAL_TIMEOUT=3s
ETCDCTL_CACERT=/tmp/ca.pem
ETCDCTL_CERT=/tmp/cert.pem
ETCDCTL_KEY=/tmp/key.pem


etcdctl --endpoints=$ENDPOINTS member list
etcdctl --write-out=table --endpoints=$ENDPOINTS endpoint status
etcdctl --write-out=table --endpoints=10.40.1.129:2379 endpoint status
etcdctl --endpoints=$ENDPOINTS endpoint health

etcdctl move-leader 7a34f6bf27d4be0b
systemctl stop etcd && rm -fr /var/lib/etcd/member/ && systemctl daemon-reload && systemctl start etcd
############################################################################

apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.2.129
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: master1
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
certSANs:
  - master1
  - node1
  - 192.168.2.129
  - 192.168.2.130
  - 127.0.0.1
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: cluster.local
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  external:
    endpoints:
    - http://192.168.2.129:2379
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: 1.22.3
networking:
  dnsDomain: cluster.local
  serviceSubnet: 10.96.0.0/12
  podSubnet: 192.168.0.0/16
scheduler:
  extraArgs:
    bind-address: 0.0.0.0
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs # iptables

kubeadm config print init-defaults >> init-defaults.yaml
kubeadm config print join-defaults >> join-defaults.yaml
kubeadm config images pull

kubeadm init --config=join-defaults.yaml --ignore-preflight-errors=...

kubeadm init --config=init-defaults.yaml
kubeadm init --pod-network-cidr=192.168.0.0/16


kubeadm join 10.40.254.151:16443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:00ee7fd8bb24a18d71c23aacc5b132539c2a4d0985128b84ae33dd8ff889e6dd \
        --control-plane

kubeadm join 192.168.2.129:6443 --token 6encpg.vhxllb5gityixefw --discovery-token-ca-cert-hash sha256:3bd6a1a3f7a31701741854b919593d2e0008bf765ad4b4d23c91ded64e1d4f8c
	
		
kubeadm token generate
kubeadm token create 6encpg.vhxllb5gityixefw --print-join-command --ttl=0



################################ control node at user ############################################
  
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

################################ control node at root ############################################
cat <<EOF | tee -a ~/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF

############################## CIDR Calico ##############################################


kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml

############################################################################

kubectl get pods --all-namespaces
kubectl get nodes -o wide

#########################	Debian/Ubuntu HAProxy packages	####################################

curl https://haproxy.debian.net/bernat.debian.org.gpg \
      | gpg --dearmor > /usr/share/keyrings/haproxy.debian.net.gpg
echo deb "[signed-by=/usr/share/keyrings/haproxy.debian.net.gpg]" \
      http://haproxy.debian.net bullseye-backports-2.4 main \
      > /etc/apt/sources.list.d/haproxy.list

apt-get update && apt-get install -y haproxy=2.4.\*

IPLOCAL=hostname -i

cat <<EOF | tee -a /etc/haproxy/haproxy.cfg
frontend k8s-api
	bind 10.40.254.135:6443
	bind 127.0.0.1:6443
	mode tcp
	option tcplog
	default_backend k8s-api

backend k8s-api
	mode tcp
	option tcplog
	option tcp-check
	balance roundrobin
	default-server port 6443 inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
        server apiserver1 10.40.254.133:6443 check
        server apiserver2 10.40.254.140:6443 check
        server apiserver3 10.40.254.149:6443 check
EOF

systemctl restart haproxy.service

netstat -alpn | grep 6443

nano  /etc/kubernetes/kubelet.conf
# nano /etc/kubernetes/bootstrap-kubelet.conf
# replace
server: https://127.0.0.1:6443

systemctl restart kubelet && systemctl restart docker


#########################	Установка панели управления Kubernetes Dashboard	####################################
https://github.com/kubernetes/dashboard

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.4.0/aio/deploy/recommended.yaml
kubectl get pods --all-namespaces -o wide | grep  kubernetes-dashboard

cat <<EOF | tee ~/dashboard-adminuser.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kube-system
EOF

kubectl create deployment nginx --image=nginx
POD_NAME=$(kubectl get pods -l app=nginx -o jsonpath="{.items[0].metadata.name}")
kubectl port-forward $POD_NAME 8080:80
kubectl logs $POD_NAME
kubectl exec -ti $POD_NAME -- nginx -v

kubectl expose deployment nginx --port 80 --type NodePort
kubectl expose deployment nginx --port 80 --type ClusterIP -n dev


kubectl apply -f dashboard-adminuser.yaml
kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
kubectl -n kube-system get secret $(kubectl -n kube-system get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
-NodePort
KUBE_EDITOR="nano" kubectl -n kube-system edit service kubernetes-dashboard
  ports:
  - nodePort: 30000
    port: 443
    protocol: TCP
    targetPort: 8443

  type: NodePort



You should see yaml representation of the service. Change type: ClusterIP to type: NodePort and save file
###################################коррекция таймаута для WEB##########################################################
KUBE_EDITOR="nano" kubectl edit deployment kubernetes-dashboard -n kubernetes-dashboard
			extraArgs:
				- --token-ttl=86400


eyJhbGciOiJSUzI1NiIsImtpZCI6IkNrVGFYMlNSaW1yLXUyTjN5b0h0ZE5xMkpzZzBVOEIzeUxHVFgyaW5FelEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLWdyeHRoIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI5NWZiYWE5Yy1jN2IwLTQ0ZWQtOTZlYS03YTUxYTM0OGQ3YTUiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.a0FYykbdpODEp0uSg2ueNVehu0yrqXiVyv6_NgeQ5VSdVkbz85mX8e9HJs5-0QhPfALWrbnDBOB6CuYl4VYwtWIBAnwwW2Q6w0BVC6Gi3OpUWqMMnINoPvyOjGqTldEvumUSRiG9oTSPa9lXx30_-u2bW1SI6MArey5LRk0-VSM7yFDycqPFyJCkhVuI40d4IlpGQjkzORnvDi_j7OHxSC1hoE0-1Yd7WWmLi7255EipO67SaPVBVqUCib3MjmgHsIBW8rhKMxiBnE5fICzs3Bw66OhHjzlY3FeRy4sMxOrlld4sDy1ZfnLEY5-hdBIZaX7Bm2BSTwrf16g4TBfGuA

- Kubernetes metrics-server Error – Readiness probe failed: HTTP probe failed with statuscode

Solution:
- Download 
wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
- Modify and add "- --kubelet-insecure-tls" in deployment.spec.template.spec.containers.args
- kubectl apply -f components.yaml
- kubectl top nodes
- kubectl top  pod kube-proxy-5p7gr -n=kube-system

#########################	Установка Ingress контроллера	####################################
- my-ingress-controller.yaml - Деплой ingress controller в режиме NodePort на помеченные при помощи специальных lables поды.
```
kubectl label nodes node2 ingress-nginx-node=enable 
```
git clone https://github.com/litr05/shpor.git
cd shpor/k8s/
kubectl apply -f ingress_deploy.yaml

#########################	Remove worker node from Kubernetes	####################################

kubectl get nodes
kubectl cordon < node-name >
kubectl drain < node-name > --ignore-daemonsets
kubectl delete node < node-name >

On Worker Node (nodetoberemoved). Remove join/init setting from node
kubeadm reset

############################### перевод ноды на Containerd #############################################

systemctl stop kubelet && systemctl stop docker
apt-get update && apt-get -y remove docker-ce docker-ce-cli
nano /etc/containerd/config.toml
containerd config default > /etc/containerd/config.toml
systemctl restart containerd

echo '--resolv-conf=/run/systemd/resolve/resolv.conf --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock"' >> /var/lib/kubelet/kubeadm-flags.env
nano /var/lib/kubelet/kubeadm-flags.env

systemctl start kubelet

############################################################################
Environment="KUBELET_EXTRA_ARGS=--runtime-cgroups=/system.slice/containerd.service --container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"

################################# NGINKS + Keepallive  ###########################################

apt install keepalived

groupadd -r keepalived_script
useradd -r -s /sbin/nologin -g keepalived_script -M keepalived_script
enable_script_security

curl https://nginx.org/keys/nginx_signing.key | gpg --dearmor \
    | tee /usr/share/keyrings/nginx-archive-keyring.gpg >/dev/null
gpg --dry-run --quiet --import --import-options import-show /usr/share/keyrings/nginx-archive-keyring.gpg

echo "deb [signed-by=/usr/share/keyrings/nginx-archive-keyring.gpg] \
http://nginx.org/packages/debian `lsb_release -cs` nginx" \
    | tee /etc/apt/sources.list.d/nginx.list

apt install nginx



###################################### Автодополнение KUBECTL ############################################################
apt-get install bash-completion

echo "source <(kubectl completion bash)" >> ~/.bashrc

###################################### TSHOT ##################################################################
kubectl get events --sort-by=.metadata.creationTimestamp -n kube-system

kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools

##############################  How to use the master node as worker node in Kubernetes cluster?  #############################################
By default, your cluster will not schedule pods on the master for security reasons. 
If you want to be able to schedule pods on the master, e.g. for a single-machine Kubernetes cluster for development, run:
```
kubectl get nodes

NAME     STATUS   ROLES    AGE   VERSION
yasin   Ready    master   11d   v1.13.4
```
kubectl taint nodes debian node-role.kubernetes.io/master-


#######################  Upgrade KUBERNETES ######################

apt-mark unhold kubeadm && \
apt-get update && apt-get install -y kubeadm=1.23.1-00 && \
apt-mark hold kubeadm

kubeadm version
kubeadm upgrade plan
sudo kubeadm upgrade apply v1.23.1
kubectl drain master1

apt-mark unhold kubelet kubectl && \
apt-get update && apt-get install -y kubelet=1.23.1-00 kubectl=1.23.1-00 && \
apt-mark hold kubelet kubectl

systemctl daemon-reload && \
systemctl restart kubelet
kubectl uncordon master1

####### SWAP ON KUBELET   ###################
echo 'Environment="KUBELET_EXTRA_ARGS=--fail-swap-on=false"' >> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

systemctl daemon-reload && systemctl restart kubelet

Если развернуто с помощью kubespray
 nano /etc/kubernetes/kubelet-config.yaml
 failSwapOn: false
reboot

Debian SWAP config
/dev/mapper/debian--vg-swap none            swap    sw              0       0

#####################  CERT MANAGER ##############################

kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml

curl -L -o cmctl.tar.gz https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cmctl-linux-amd64.tar.gz

tar xzf cmctl.tar.gz
mv cmctl /usr/local/bin

kubectl -n kubernetes-dashboard create secret tls kube-ca-secret \
--cert=/etc/kubernetes/pki/ca.crt \
--key=/etc/kubernetes/pki/ca.key

root@master1:~# kubectl apply -f cert.yaml
issuer.cert-manager.io/monitoring-issuer created
certificate.cert-manager.io/mon-certificate created

kubectl -n kubernetes-dashboard get issuers
kubectl -n kubernetes-dashboard get certificate